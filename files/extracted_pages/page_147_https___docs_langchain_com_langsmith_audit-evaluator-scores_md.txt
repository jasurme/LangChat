Published Time: Thu, 12 Feb 2026 11:46:15 GMT

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.langchain.com/llms.txt
> Use this file to discover all available pages before exploring further.

# How to audit evaluator scores

LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.

## In the comparison view

In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the "edit" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under "Make correction". If you would like, you may also attach an explanation to your correction. This is useful if you are using a [few-shot evaluator](/langsmith/create-few-shot-evaluators) and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.

![Image 1: Audit Evaluator Comparison View](https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5b815b771c18f291a9ef1b7defb9feb3)

## In the runs table

In the runs table, find the "Feedback" column and click on the feedback tag to bring up the feedback details. Again, click the "edit" icon on the right to bring up the corrections view.

![Image 2: Audit Evaluator Runs Table](https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e64530681ac9125751af2383b67ba35)

## In the SDK

Corrections can be made via the SDK's `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.

 ```python Python theme={null} import langsmith client = langsmith.Client() client.update_feedback( my_feedback_id, correction={ "score": 1, }, ) ``` ```typescript TypeScript theme={null} import { Client } from 'langsmith'; const client = new Client(); await client.updateFeedback( myFeedbackId, { correction: { score: 1, } } ) ``` 

***

 [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/audit-evaluator-scores.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose). 

 [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
