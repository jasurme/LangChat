Published Time: Thu, 12 Feb 2026 11:54:11 GMT

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.langchain.com/llms.txt
> Use this file to discover all available pages before exploring further.

# Trace Semantic Kernel applications

LangSmith can capture traces generated by [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) using its built-in OpenTelemetry support. This guide shows you how to automatically capture traces from your Semantic Kernel applications and send them to LangSmith for monitoring and analysis.

## Installation

Install the required packages using your preferred package manager:

 ```bash pip theme={null} pip install langsmith semantic-kernel opentelemetry-instrumentation-openai ``` ```bash uv theme={null} uv add langsmith semantic-kernel opentelemetry-instrumentation-openai ``` 

## Setup

### 1. Configure environment variables

Set your [API keys](/langsmith/create-account-api-key) and project name:

```bash  theme={null}
export LANGSMITH_API_KEY= export LANGSMITH_PROJECT= export OPENAI_API_KEY= ``` ### 2. Configure OpenTelemetry integration In your Semantic Kernel application, configure the LangSmith OpenTelemetry integration along with the OpenAI instrumentor: ```python theme={null} from langsmith.integrations.otel import configure from opentelemetry.instrumentation.openai import OpenAIInstrumentor # Configure LangSmith tracing configure(project_name="semantic-kernel-demo") # Instrument OpenAI calls OpenAIInstrumentor().instrument() ```  You do not need to set any OpenTelemetry environment variables or configure exporters manuallyâ€”`configure()` handles everything automatically.  ### 3. Create and run your Semantic Kernel application Once configured, your Semantic Kernel application will automatically send traces to LangSmith: ```python theme={null} import asyncio from semantic_kernel import Kernel from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig from langsmith.integrations.otel import configure from opentelemetry.instrumentation.openai import OpenAIInstrumentor # Configure LangSmith tracing configure(project_name="semantic-kernel-assistant") # Instrument OpenAI calls OpenAIInstrumentor().instrument() # Configure Semantic Kernel kernel = Kernel() kernel.add_service(OpenAIChatCompletion()) # Create a prompt template code_analysis_prompt = """ Analyze the following code and provide insights: Code: {{$code}} Please provide: 1. A brief summary of what the code does 2. Any potential improvements 3. Code quality assessment """ prompt_template_config = PromptTemplateConfig( template=code_analysis_prompt, name="code_analyzer", template_format="semantic-kernel", input_variables=[ InputVariable(name="code", description="The code to analyze", is_required=True), ], ) # Add the function to the kernel code_analyzer = kernel.add_function( function_name="analyzeCode", plugin_name="codeAnalysisPlugin", prompt_template_config=prompt_template_config, ) async def main(): sample_code = """ def fibonacci(n): if n <= 1: return n return fibonacci(n-1) + fibonacci(n-2) """ result = await kernel.invoke(code_analyzer, code=sample_code) print("Code Analysis:") print(result) if __name__ == "__main__": asyncio.run(main()) ``` ## Advanced usage ### Custom metadata and tags You can add custom metadata to your traces by setting span attributes: ```python theme={null} from opentelemetry import trace tracer = trace.get_tracer(__name__) async def analyze_with_metadata(code: str): with tracer.start_as_current_span("semantic_kernel_workflow") as span: span.set_attribute("langsmith.metadata.workflow_type", "code_analysis") span.set_attribute("langsmith.metadata.user_id", "developer_123") span.set_attribute("langsmith.span.tags", "semantic-kernel,code-analysis") result = await kernel.invoke(code_analyzer, code=code) return result ``` ### Combining with other instrumentors You can combine Semantic Kernel tracing with other OpenTelemetry instrumentors: ```python theme={null} from opentelemetry.instrumentation.openai import OpenAIInstrumentor from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor # Initialize multiple instrumentors OpenAIInstrumentor().instrument() HTTPXClientInstrumentor().instrument() ``` ## Resources * [Semantic Kernel documentation](https://learn.microsoft.com/en-us/semantic-kernel/overview/) * [Semantic Kernel observability guide](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/) * [LangSmith OpenTelemetry guide](/langsmith/trace-with-opentelemetry) ***  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-semantic-kernel.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
