Published Time: Thu, 12 Feb 2026 06:03:07 GMT

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.langchain.com/llms.txt
> Use this file to discover all available pages before exploring further.

# Messages

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.

Messages are objects that contain:

*  [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`) *  [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.) *  [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called. ## Basic usage The simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/javascript/langchain/models#invocation). ```typescript theme={null} import { initChatModel, HumanMessage, SystemMessage } from "langchain"; const model = await initChatModel("gpt-5-nano"); const systemMsg = new SystemMessage("You are a helpful assistant."); const humanMsg = new HumanMessage("Hello, how are you?"); const messages = [systemMsg, humanMsg]; const response = await model.invoke(messages); // Returns AIMessage ``` ### Text prompts Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history. ```typescript theme={null} const response = await model.invoke("Write a haiku about spring"); ``` **Use text prompts when:** * You have a single, standalone request * You don't need conversation history * You want minimal code complexity ### Message prompts Alternatively, you can pass in a list of messages to the model by providing a list of message objects. ```typescript theme={null} import { SystemMessage, HumanMessage, AIMessage } from "langchain"; const messages = [ new SystemMessage("You are a poetry expert"), new HumanMessage("Write a haiku about spring"), new AIMessage("Cherry blossoms bloom..."), ]; const response = await model.invoke(messages); ``` **Use message prompts when:** * Managing multi-turn conversations * Working with multimodal content (images, audio, files) * Including system instructions ### Dictionary format You can also specify messages directly in OpenAI chat completions format. ```typescript theme={null} const messages = [ { role: "system", content: "You are a poetry expert" }, { role: "user", content: "Write a haiku about spring" }, { role: "assistant", content: "Cherry blossoms bloom..." }, ]; const response = await model.invoke(messages); ``` ## Message types *  [System message](#system-message) - Tells the model how to behave and provide context for interactions *  [Human message](#human-message) - Represents user input and interactions with the model *  [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata *  [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/javascript/langchain/models#tool-calling) ### System message A [`SystemMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.SystemMessage.html) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses. ```typescript Basic instructions theme={null} import { SystemMessage, HumanMessage, AIMessage } from "langchain"; const systemMsg = new SystemMessage("You are a helpful coding assistant."); const messages = [ systemMsg, new HumanMessage("How do I create a REST API?"), ]; const response = await model.invoke(messages); ``` ```typescript Detailed persona theme={null} import { SystemMessage, HumanMessage } from "langchain"; const systemMsg = new SystemMessage(` You are a senior TypeScript developer with expertise in web frameworks. Always provide code examples and explain your reasoning. Be concise but thorough in your explanations. `); const messages = [ systemMsg, new HumanMessage("How do I create a REST API?"), ]; const response = await model.invoke(messages); ``` *** ### Human message A [`HumanMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.HumanMessage.html) represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content). #### Text content ```typescript Message object theme={null} const response = await model.invoke([ new HumanMessage("What is machine learning?"), ]); ``` ```typescript String shortcut theme={null} const response = await model.invoke("What is machine learning?"); ``` #### Message metadata ```typescript Add metadata theme={null} const humanMsg = new HumanMessage({ content: "Hello!", name: "alice", id: "msg_123", }); ```  The `name` field behavior varies by provider – some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).  *** ### AI message An [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access. ```typescript theme={null} const response = await model.invoke("Explain AI"); console.log(typeof response); // AIMessage ``` [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) objects are returned by the model when calling it, which contains all of the associated metadata in the response. Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) object and insert it into the message history as if it came from the model. ```typescript theme={null} import { AIMessage, SystemMessage, HumanMessage } from "langchain"; const aiMsg = new AIMessage("I'd be happy to help you with that question!"); const messages = [ new SystemMessage("You are a helpful assistant"), new HumanMessage("Can you help me?"), aiMsg, // Insert as if it came from the model new HumanMessage("Great! What's 2+2?") ] const response = await model.invoke(messages); ```   The text content of the message.   The raw content of the message.   The standardized content blocks of the message. (See [content](#message-content))   The tool calls made by the model. Empty if no tools are called.   A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)   The usage metadata of the message, which can contain token counts when available. See [`UsageMetadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html).   The response metadata of the message.  #### Tool calls When models make [tool calls](/oss/javascript/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html): ```typescript theme={null} const modelWithTools = model.bindTools([getWeather]); const response = await modelWithTools.invoke("What's the weather in Paris?"); for (const toolCall of response.tool_calls) { console.log(`Tool: ${toolCall.name}`); console.log(`Args: ${toolCall.args}`); console.log(`ID: ${toolCall.id}`); } ``` Other structured data, such as reasoning or citations, can also appear in message [content](/oss/javascript/langchain/messages#message-content). #### Token usage An [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html) field: ```typescript theme={null} import { initChatModel } from "langchain"; const model = await initChatModel("gpt-5-nano"); const response = await model.invoke("Hello!"); console.log(response.usage_metadata); ``` ```json theme={null} { "output_tokens": 304, "input_tokens": 8, "total_tokens": 312, "input_token_details": { "cache_read": 0 }, "output_token_details": { "reasoning": 256 } } ``` See [`UsageMetadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html) for details. #### Streaming and chunks During streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessageChunk.html) objects that can be combined into a full message object:  ```typescript theme={null} import { AIMessageChunk } from "langchain"; let finalChunk: AIMessageChunk | undefined; for (const chunk of chunks) { finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk; } ```  Learn more: * [Streaming tokens from chat models](/oss/javascript/langchain/models#stream) * [Streaming tokens and/or steps from agents](/oss/javascript/langchain/streaming)  *** ### Tool message For models that support [tool calling](/oss/javascript/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model. [Tools](/oss/javascript/langchain/tools) can generate [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/javascript/langchain/tools). ```typescript theme={null} import { AIMessage, ToolMessage } from "langchain"; const aiMessage = new AIMessage({ content: [], tool_calls: [{ name: "get_weather", args: { location: "San Francisco" }, id: "call_123" }] }); const toolMessage = new ToolMessage({ content: "Sunny, 72°F", tool_call_id: "call_123" }); const messages = [ new HumanMessage("What's the weather in San Francisco?"), aiMessage, // Model's tool call toolMessage, // Tool execution result ]; const response = await model.invoke(messages); // Model processes the result ```   The stringified output of the tool call.   The ID of the tool call that this message is responding to. Must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html).   The name of the tool that was called.   Additional data not sent to the model but can be accessed programmatically.  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.  For example, a [retrieval](/oss/javascript/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below: ```typescript theme={null} import { ToolMessage } from "langchain"; // Artifact available downstream const artifact = { document_id: "doc_123", page: 0 }; const toolMessage = new ToolMessage({ content: "It was the best of times, it was the worst of times.", tool_call_id: "call_123", name: "search_books", artifact }); ``` See the [RAG tutorial](/oss/javascript/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/javascript/langchain/agents) with LangChain.  *** ## Message content You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data. Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below. LangChain chat models accept message content in the `content` attribute. This may contain either: 1. A string 2. A list of content blocks in a provider-native format 3. A list of [LangChain's standard content blocks](#standard-content-blocks) See below for an example using [multimodal](#multimodal) inputs: ```typescript theme={null} import { HumanMessage } from "langchain"; // String content const humanMessage = new HumanMessage("Hello, how are you?"); // Provider-native format (e.g., OpenAI) const humanMessage = new HumanMessage({ content: [ { type: "text", text: "Hello, how are you?" }, { type: "image_url", image_url: { url: "https://example.com/image.jpg" }, }, ], }); // List of standard content blocks const humanMessage = new HumanMessage({ contentBlocks: [ { type: "text", text: "Hello, how are you?" }, { type: "image", url: "https://example.com/image.jpg" }, ], }); ``` ### Standard content blocks LangChain provides a standard representation for message content that works across providers. Message objects implement a `contentBlocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/javascript/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/javascript/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:   ```typescript theme={null} import { AIMessage } from "@langchain/core/messages"; const message = new AIMessage({ content: [ { "type": "thinking", "thinking": "...", "signature": "WaUjzkyp...", }, { "type":"text", "text": "...", "id": "msg_abc123", }, ], response_metadata: { model_provider: "anthropic" }, }); console.log(message.contentBlocks); ```   ```typescript theme={null} import { AIMessage } from "@langchain/core/messages"; const message = new AIMessage({ content: [ { "type": "reasoning", "id": "rs_abc123", "summary": [ {"type": "summary_text", "text": "summary 1"}, {"type": "summary_text", "text": "summary 2"}, ], }, {"type": "text", "text": "..."}, ], response_metadata: { model_provider: "openai" }, }); console.log(message.contentBlocks); ```  See the [integrations guides](/oss/javascript/integrations/providers/overview) to get started with the inference provider of your choice.  **Serializing standard content** If an application outside of LangChain needs access to the standard content block representation, you can opt-in to storing content blocks in message content. To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or, initialize any chat model with `outputVersion: "v1"`: ```typescript theme={null} import { initChatModel } from "langchain"; const model = await initChatModel( "gpt-5-nano", { outputVersion: "v1" } ); ```  ### Multimodal **Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers. [Chat models](/oss/javascript/langchain/models) can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data.  Extra keys can be included top-level in the content block or nested in `"extras": {"key": value}`. [OpenAI](/oss/javascript/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/javascript/integrations/chat/bedrock), for example, require a filename for PDFs. See the [provider page](/oss/javascript/integrations/providers/overview) for your chosen model for specifics.  ```typescript Image input theme={null} // From URL const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this image." }, { type: "image", source_type: "url", url: "https://example.com/path/to/image.jpg" }, ], }); // From base64 data const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this image." }, { type: "image", source_type: "base64", data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...", }, ], }); // From provider-managed File ID const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this image." }, { type: "image", source_type: "id", id: "file-abc123" }, ], }); ``` ```typescript PDF document input theme={null} // From URL const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this document." }, { type: "file", source_type: "url", url: "https://example.com/path/to/document.pdf", mime_type: "application/pdf" }, ], }); // From base64 data const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this document." }, { type: "file", source_type: "base64", data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...", mime_type: "application/pdf", }, ], }); // From provider-managed File ID const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this document." }, { type: "file", source_type: "id", id: "file-abc123" }, ], }); ``` ```typescript Audio input theme={null} // From base64 data const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this audio." }, { type: "audio", source_type: "base64", data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...", }, ], }); // From provider-managed File ID const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this audio." }, { type: "audio", source_type: "id", id: "file-abc123" }, ], }); ``` ```typescript Video input theme={null} // From base64 data const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this video." }, { type: "video", source_type: "base64", data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...", }, ], }); // From provider-managed File ID const message = new HumanMessage({ content: [ { type: "text", text: "Describe the content of this video." }, { type: "video", source_type: "id", id: "file-abc123" }, ], }); ```  Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.  ### Content block reference Content blocks are represented (either when creating a message or accessing the `contentBlocks` field) as a list of typed objects. Each item in the list must adhere to one of the following block types:     **Purpose:** Standard text output  Always `"text"`   The text content   List of annotations for the text  **Example:** ```typescript theme={null} { type: "text", text: "Hello world", annotations: [] } ```   **Purpose:** Model reasoning steps  Always `"reasoning"`   The reasoning content  **Example:** ```typescript theme={null} { type: "reasoning", reasoning: "The user is asking about..." } ```       **Purpose:** Image data  Always `"image"`   URL pointing to the image location.   Base64-encoded image data.   Reference to the image in an external file storage system (e.g., OpenAI or Anthropic's Files API).   Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`). Required for base64 data.    **Purpose:** Audio data  Always `"audio"`   URL pointing to the audio location.   Base64-encoded audio data.   Reference to the audio file in an external file storage system (e.g., OpenAI or Anthropic's Files API).   Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.    **Purpose:** Video data  Always `"video"`   URL pointing to the video location.   Base64-encoded video data.   Reference to the video file in an external file storage system (e.g., OpenAI or Anthropic's Files API).   Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`). Required for base64 data.    **Purpose:** Generic files (PDF, etc)  Always `"file"`   URL pointing to the file location.   Base64-encoded file data.   Reference to the file in an external file storage system (e.g., OpenAI or Anthropic's Files API).   File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`). Required for base64 data.    **Purpose:** Document text (`.txt`, `.md`)  Always `"text-plain"`   The text content   Title of the text content   [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)        **Purpose:** Function calls  Always `"tool_call"`   Name of the tool to call   Arguments to pass to the tool   Unique identifier for this tool call  **Example:** ```typescript theme={null} { type: "tool_call", name: "search", args: { query: "weather" }, id: "call_123" } ```   **Purpose:** Streaming tool fragments  Always `"tool_call_chunk"`   Name of the tool being called   Partial tool arguments (may be incomplete JSON)   Tool call identifier   Position of this chunk in the stream    **Purpose:** Malformed calls  Always `"invalid_tool_call"`   Name of the tool that failed to be called   Raw arguments that failed to parse   Description of what went wrong  **Common errors:** Invalid JSON, missing required fields       **Purpose:** Tool call that is executed server-side.  Always `"server_tool_call"`   An identifier associated with the tool call.   The name of the tool to be called.   Partial tool arguments (may be incomplete JSON)    **Purpose:** Streaming server-side tool call fragments  Always `"server_tool_call_chunk"`   An identifier associated with the tool call.   Name of the tool being called   Partial tool arguments (may be incomplete JSON)   Position of this chunk in the stream    **Purpose:** Search results  Always `"server_tool_result"`   Identifier of the corresponding server tool call.   Identifier associated with the server tool result.   Execution status of the server-side tool. `"success"` or `"error"`.   Output of the executed tool.       **Purpose:** Provider-specific escape hatch  Always `"non_standard"`   Provider-specific data structure  **Usage:** For experimental or provider-unique features  Additional provider-specific content types may be found within the [reference documentation](/oss/javascript/integrations/providers/overview) of each model provider.  Each of these content blocks mentioned above are indvidually addressable as types when importing the [`ContentBlock`](https://reference.langchain.com/javascript/types/_langchain_core.messages.MessageContent.html) type. ```typescript theme={null} import { ContentBlock } from "langchain"; // Text block const textBlock: ContentBlock.Text = { type: "text", text: "Hello world", } // Image block const imageBlock: ContentBlock.Multimodal.Image = { type: "image", url: "https://example.com/image.png", mimeType: "image/png", } ```  View the canonical type definitions in the [API reference](https://reference.langchain.com/javascript/modules/_langchain_core.messages.html).  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the [`content`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.BaseMessage.html#content) property, but rather a new property that can be used to access the content of a message in a standardized format.  ## Use with chat models [Chat models](/oss/javascript/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages. Refer to the below guides to learn more: * Built-in features for [persisting and managing conversation histories](/oss/javascript/langchain/short-term-memory) * Strategies for managing context windows, including [trimming and summarizing messages](/oss/javascript/langchain/short-term-memory#common-patterns) ***  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
