Published Time: Thu, 12 Feb 2026 08:42:16 GMT

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.langchain.com/llms.txt
> Use this file to discover all available pages before exploring further.

# Build a multi-source knowledge base with routing

## Overview

The **router pattern** is a [multi-agent](/oss/javascript/langchain/multi-agent) architecture where a routing step classifies input and directs it to specialized agents, with results synthesized into a combined response. This pattern excels when your organization's knowledge lives across distinct **verticals**â€”separate knowledge domains that each require their own agent with specialized tools and prompts.

In this tutorial, you'll build a multi-source knowledge base router that demonstrates these benefits through a realistic enterprise scenario. The system will coordinate three specialists:

* A **GitHub agent** that searches code, issues, and pull requests.
* A **Notion agent** that searches internal documentation and wikis.
* A **Slack agent** that searches relevant threads and discussions.

When a user asks "How do I authenticate API requests?", the router decomposes the query into source-specific sub-questions, routes them to the relevant agents in parallel, and synthesizes results into a coherent answer.

```mermaid  theme={null}
graph LR
    A([Query]) --> B[Classify]
    B --> C[GitHub agent]
    B --> D[Notion agent]
    B --> E[Slack agent]
    C --> F[Synthesize]
    D --> F
    E --> F
    F --> G([Combined answer])
```

### Why use a router?

The router pattern provides several advantages:

* **Parallel execution**: Query multiple sources simultaneously, reducing latency compared to sequential approaches.
* **Specialized agents**: Each vertical has focused tools and prompts optimized for its domain.
* **Selective routing**: Not every query needs every sourceâ€”the router intelligently selects relevant verticals.
* **Targeted sub-questions**: Each agent receives a question tailored to its domain, improving result quality.
* **Clean synthesis**: Results from multiple sources are combined into a single, coherent response.

### Concepts

We will cover the following concepts:

* [Multi-agent systems](/oss/javascript/langchain/multi-agent)
* [StateGraph](/oss/javascript/langchain/graphs) for workflow orchestration
* [Send API](/oss/javascript/langchain/send) for parallel execution

 **Router vs. Subagents**: The [subagents pattern](/oss/javascript/langchain/multi-agent/subagents) can also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically. 

## Setup

### Installation

This tutorial requires the `langchain` and `langgraph` packages:

 ```bash npm theme={null} npm install langchain @langchain/langgraph ``` ```bash yarn theme={null} yarn add langchain @langchain/langgraph ``` ```bash pnpm theme={null} pnpm add langchain @langchain/langgraph ``` 

For more details, see our [Installation guide](/oss/javascript/langchain/install).

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

 ```bash bash theme={null} export LANGSMITH_TRACING="true" export LANGSMITH_API_KEY="..." ``` ```typescript typescript theme={null} process.env.LANGSMITH_TRACING = "true"; process.env.LANGSMITH_API_KEY = "..."; ``` 

### Select an LLM

Select a chat model from LangChain's suite of integrations:

  ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)  ```bash npm theme={null} npm install @langchain/openai ``` ```bash pnpm theme={null} pnpm install @langchain/openai ``` ```bash yarn theme={null} yarn add @langchain/openai ``` ```bash bun theme={null} bun add @langchain/openai ```   ```typescript initChatModel theme={null} import { initChatModel } from "langchain"; process.env.OPENAI_API_KEY = "your-api-key"; const model = await initChatModel("gpt-4.1"); ``` ```typescript Model Class theme={null} import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4.1", apiKey: "your-api-key" }); ```    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)  ```bash npm theme={null} npm install @langchain/anthropic ``` ```bash pnpm theme={null} pnpm install @langchain/anthropic ``` ```bash yarn theme={null} yarn add @langchain/anthropic ``` ```bash pnpm theme={null} pnpm add @langchain/anthropic ```   ```typescript initChatModel theme={null} import { initChatModel } from "langchain"; process.env.ANTHROPIC_API_KEY = "your-api-key"; const model = await initChatModel("claude-sonnet-4-5-20250929"); ``` ```typescript Model Class theme={null} import { ChatAnthropic } from "@langchain/anthropic"; const model = new ChatAnthropic({ model: "claude-sonnet-4-5-20250929", apiKey: "your-api-key" }); ```    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)  ```bash npm theme={null} npm install @langchain/azure ``` ```bash pnpm theme={null} pnpm install @langchain/azure ``` ```bash yarn theme={null} yarn add @langchain/azure ``` ```bash bun theme={null} bun add @langchain/azure ```   ```typescript initChatModel theme={null} import { initChatModel } from "langchain"; process.env.AZURE_OPENAI_API_KEY = "your-api-key"; process.env.AZURE_OPENAI_ENDPOINT = "your-endpoint"; process.env.OPENAI_API_VERSION = "your-api-version"; const model = await initChatModel("azure_openai:gpt-4.1"); ``` ```typescript Model Class theme={null} import { AzureChatOpenAI } from "@langchain/openai"; const model = new AzureChatOpenAI({ model: "gpt-4.1", azureOpenAIApiKey: "your-api-key", azureOpenAIApiEndpoint: "your-endpoint", azureOpenAIApiVersion: "your-api-version" }); ```    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)  ```bash npm theme={null} npm install @langchain/google-genai ``` ```bash pnpm theme={null} pnpm install @langchain/google-genai ``` ```bash yarn theme={null} yarn add @langchain/google-genai ``` ```bash bun theme={null} bun add @langchain/google-genai ```   ```typescript initChatModel theme={null} import { initChatModel } from "langchain"; process.env.GOOGLE_API_KEY = "your-api-key"; const model = await initChatModel("google-genai:gemini-2.5-flash-lite"); ``` ```typescript Model Class theme={null} import { ChatGoogleGenerativeAI } from "@langchain/google-genai"; const model = new ChatGoogleGenerativeAI({ model: "gemini-2.5-flash-lite", apiKey: "your-api-key" }); ```    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)  ```bash npm theme={null} npm install @langchain/aws ``` ```bash pnpm theme={null} pnpm install @langchain/aws ``` ```bash yarn theme={null} yarn add @langchain/aws ``` ```bash bun theme={null} bun add @langchain/aws ```   ```typescript initChatModel theme={null} import { initChatModel } from "langchain"; // Follow the steps here to configure your credentials: // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html const model = await initChatModel("bedrock:gpt-4.1"); ``` ```typescript Model Class theme={null} import { ChatBedrockConverse } from "@langchain/aws"; // Follow the steps here to configure your credentials: // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html const model = new ChatBedrockConverse({ model: "gpt-4.1", region: "us-east-2" }); ```  

## 1. Define state

First, define the state schemas. We use three types:

* **`AgentInput`**: Simple state passed to each subagent (just a query)
* **`AgentOutput`**: Result returned by each subagent (source name + result)
* **`RouterState`**: Main workflow state tracking the query, classifications, results, and final answer

```typescript  theme={null}
import { StateSchema, ReducedValue } from "@langchain/langgraph";
import { z } from "zod/v4";

const AgentOutput = z.object({
  source: z.string(),
  result: z.string(),
});

const RouterState = new StateSchema({
  query: z.string(),
  classifications: z.array(
    z.object({
      source: z.enum(["github", "notion", "slack"]),
      query: z.string(),
    })
  ),
  results: new ReducedValue(
    z.array(AgentOutput).default(() => []),
    { reducer: (current, update) => current.concat(update) }
  ),
  finalAnswer: z.string(),
});
```

The `results` field uses a **reducer** (`operator.add` in Python, a concat function in JS) to collect outputs from parallel agent executions into a single list.

## 2. Define tools for each vertical

Create tools for each knowledge domain. In a production system, these would call actual APIs. For this tutorial, we use stub implementations that return mock data. We define 7 tools across 3 verticals: GitHub (search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).

```typescript expandable theme={null}
import { tool } from "langchain";
import { z } from "zod";

const searchCode = tool(
  async ({ query, repo }) => {
    return `Found code matching '${query}' in ${repo || "main"}: authentication middleware in src/auth.py`;
  },
  {
    name: "search_code",
    description: "Search code in GitHub repositories.",
    schema: z.object({
      query: z.string(),
      repo: z.string().optional().default("main"),
    }),
  }
);

const searchIssues = tool(
  async ({ query }) => {
    return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`;
  },
  {
    name: "search_issues",
    description: "Search GitHub issues and pull requests.",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const searchPrs = tool(
  async ({ query }) => {
    return `PR #156 added JWT authentication, PR #178 updated OAuth scopes`;
  },
  {
    name: "search_prs",
    description: "Search pull requests for implementation details.",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const searchNotion = tool(
  async ({ query }) => {
    return `Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens`;
  },
  {
    name: "search_notion",
    description: "Search Notion workspace for documentation.",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const getPage = tool(
  async ({ pageId }) => {
    return `Page content: Step-by-step authentication setup instructions`;
  },
  {
    name: "get_page",
    description: "Get a specific Notion page by ID.",
    schema: z.object({
      pageId: z.string(),
    }),
  }
);

const searchSlack = tool(
  async ({ query }) => {
    return `Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'`;
  },
  {
    name: "search_slack",
    description: "Search Slack messages and threads.",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const getThread = tool(
  async ({ threadId }) => {
    return `Thread discusses best practices for API key rotation`;
  },
  {
    name: "get_thread",
    description: "Get a specific Slack thread.",
    schema: z.object({
      threadId: z.string(),
    }),
  }
);
```

## 3. Create specialized agents

Create an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same patternâ€”only the tools and system prompt differ.

```typescript expandable theme={null}
import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({ model: "gpt-4.1" });

const githubAgent = createAgent({
  model: llm,
  tools: [searchCode, searchIssues, searchPrs],
  systemPrompt: `
You are a GitHub expert. Answer questions about code,
API references, and implementation details by searching
repositories, issues, and pull requests.
  `.trim(),
});

const notionAgent = createAgent({
  model: llm,
  tools: [searchNotion, getPage],
  systemPrompt: `
You are a Notion expert. Answer questions about internal
processes, policies, and team documentation by searching
the organization's Notion workspace.
  `.trim(),
});

const slackAgent = createAgent({
  model: llm,
  tools: [searchSlack, getThread],
  systemPrompt: `
You are a Slack expert. Answer questions by searching
relevant threads and discussions where team members have
shared knowledge and solutions.
  `.trim(),
});
```

## 4. Build the router workflow

Now build the router workflow using a StateGraph. The workflow has four main steps:

1. **Classify**: Analyze the query and determine which agents to invoke with what sub-questions
2. **Route**: Fan out to selected agents in parallel using `Send`
3. **Query agents**: Each agent receives a simple `AgentInput` and returns an `AgentOutput`
4. **Synthesize**: Combine collected results into a coherent response

```typescript  theme={null}
import { StateGraph, START, END, Send } from "@langchain/langgraph";
import { z } from "zod";

const routerLlm = new ChatOpenAI({ model: "gpt-4.1-mini" });

// Define structured output schema for the classifier
const ClassificationResultSchema = z.object({  // [!code highlight]
  classifications: z.array(z.object({
    source: z.enum(["github", "notion", "slack"]),
    query: z.string(),
  })).describe("List of agents to invoke with their targeted sub-questions"),
});

async function classifyQuery(state: typeof RouterState.State) {
  const structuredLlm = routerLlm.withStructuredOutput(ClassificationResultSchema);  // [!code highlight]

  const result = await structuredLlm.invoke([
    {
      role: "system",
      content: `Analyze this query and determine which knowledge bases to consult.
For each relevant source, generate a targeted sub-question optimized for that source.

Available sources:
- github: Code, API references, implementation details, issues, pull requests
- notion: Internal documentation, processes, policies, team wikis
- slack: Team discussions, informal knowledge sharing, recent conversations

Return ONLY the sources that are relevant to the query. Each source should have
a targeted sub-question optimized for that specific knowledge domain.

Example for "How do I authenticate API requests?":
- github: "What authentication code exists? Search for auth middleware, JWT handling"
- notion: "What authentication documentation exists? Look for API auth guides"
(slack omitted because it's not relevant for this technical question)`
    },
    { role: "user", content: state.query }
  ]);

  return { classifications: result.classifications };
}

function routeToAgents(state: typeof RouterState.State): Send[] {
  return state.classifications.map(
    (c) => new Send(c.source, { query: c.query })  // [!code highlight]
  );
}

async function queryGithub(state: AgentInput) {
  const result = await githubAgent.invoke({
    messages: [{ role: "user", content: state.query }]  // [!code highlight]
  });
  return { results: [{ source: "github", result: result.messages.at(-1)?.content }] };
}

async function queryNotion(state: AgentInput) {
  const result = await notionAgent.invoke({
    messages: [{ role: "user", content: state.query }]  // [!code highlight]
  });
  return { results: [{ source: "notion", result: result.messages.at(-1)?.content }] };
}

async function querySlack(state: AgentInput) {
  const result = await slackAgent.invoke({
    messages: [{ role: "user", content: state.query }]  // [!code highlight]
  });
  return { results: [{ source: "slack", result: result.messages.at(-1)?.content }] };
}

async function synthesizeResults(state: typeof RouterState.State) {
  if (state.results.length === 0) {
    return { finalAnswer: "No results found from any knowledge source." };
  }

  // Format results for synthesis
  const formatted = state.results.map(
    (r) => `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\n${r.result}`
  );

  const synthesisResponse = await routerLlm.invoke([
    {
      role: "system",
      content: `Synthesize these search results to answer the original question: "${state.query}"

- Combine information from multiple sources without redundancy
- Highlight the most relevant and actionable information
- Note any discrepancies between sources
- Keep the response concise and well-organized`
    },
    { role: "user", content: formatted.join("\n\n") }
  ]);

  return { finalAnswer: synthesisResponse.content };
}
```

## 5. Compile the workflow

Now assemble the workflow by connecting nodes with edges. The key is using `add_conditional_edges` with the routing function to enable parallel execution:

```typescript  theme={null}
const workflow = new StateGraph(RouterState)
  .addNode("classify", classifyQuery)
  .addNode("github", queryGithub)
  .addNode("notion", queryNotion)
  .addNode("slack", querySlack)
  .addNode("synthesize", synthesizeResults)
  .addEdge(START, "classify")
  .addConditionalEdges("classify", routeToAgents, ["github", "notion", "slack"])
  .addEdge("github", "synthesize")
  .addEdge("notion", "synthesize")
  .addEdge("slack", "synthesize")
  .addEdge("synthesize", END)
  .compile();
```

The `add_conditional_edges` call connects the classify node to the agent nodes through the `route_to_agents` function. When `route_to_agents` returns multiple `Send` objects, those nodes execute in parallel.

## 6. Use the router

Test your router with queries that span multiple knowledge domains:

```typescript  theme={null}
const result = await workflow.invoke({
  query: "How do I authenticate API requests?"
});

console.log("Original query:", result.query);
console.log("\nClassifications:");
for (const c of result.classifications) {
  console.log(`  ${c.source}: ${c.query}`);
}
console.log("\n" + "=".repeat(60) + "\n");
console.log("Final Answer:");
console.log(result.finalAnswer);
```

Expected output:

```
Original query: How do I authenticate API requests?

Classifications:
  github: What authentication code exists? Search for auth middleware, JWT handling
  notion: What authentication documentation exists? Look for API auth guides

============================================================

Final Answer:
To authenticate API requests, you have several options:

1. **JWT Tokens**: The recommended approach for most use cases.
   Implementation details are in `src/auth.py` (PR #156).

2. **OAuth2 Flow**: For third-party integrations, follow the OAuth2
   flow documented in Notion's 'API Authentication Guide'.

3. **API Keys**: For server-to-server communication, use Bearer tokens
   in the Authorization header.

For token refresh handling, see issue #203 and PR #178 for the latest
OAuth scope updates.
```

The router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.

## 7. Understanding the architecture

The router workflow follows a clear pattern:

### Classification phase

The `classify_query` function uses **structured output** to analyze the user's query and determine which agents to invoke. This is where the routing intelligence lives:

* Uses a Pydantic model (Python) or Zod schema (JS) to ensure valid output
* Returns a list of `Classification` objects, each with a `source` and targeted `query`
* Only includes relevant sourcesâ€”irrelevant ones are simply omitted

This structured approach is more reliable than free-form JSON parsing and makes the routing logic explicit.

### Parallel execution with send

The `route_to_agents` function maps classifications to `Send` objects. Each `Send` specifies the target node and the state to pass:

```typescript  theme={null}
// Classifications: [{ source: "github", query: "..." }, { source: "notion", query: "..." }]
// Becomes:
[new Send("github", { query: "..." }), new Send("notion", { query: "..." })]
// Both agents execute simultaneously, each receiving only the query it needs
```

Each agent node receives a simple `AgentInput` with just a `query` fieldâ€”not the full router state. This keeps the interface clean and explicit.

### Result collection with reducers

Agent results flow back to the main state via a **reducer**. Each agent returns:

```typescript  theme={null}
{ results: [{ source: "github", result: "..." }] }
```

The reducer (`operator.add` in Python) concatenates these lists, collecting all parallel results into `state["results"]`.

### Synthesis phase

After all agents complete, the `synthesize_results` function iterates over the collected results:

* Waits for all parallel branches to complete (LangGraph handles this automatically)
* References the original query to ensure the answer addresses what the user asked
* Combines information from all sources without redundancy

 **Partial results**: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the [map-reduce guide](/oss/javascript/langchain/map-reduce). 

## 8. Complete working example

Here's everything together in a runnable script:

 ```typescript theme={null} /** * Multi-Source Knowledge Router Example * * This example demonstrates the router pattern for multi-agent systems. * A router classifies queries, routes them to specialized agents in parallel, * and synthesizes results into a combined response. */ import { z } from "zod/v4"; import { tool } from "langchain"; import { StateGraph, START, END, Send, StateSchema, ReducedValue } from "@langchain/langgraph"; const AgentOutput = z.object({ source: z.string(), result: z.string(), }); const RouterState = new StateSchema({ query: z.string(), classifications: z.array( z.object({ source: z.enum(["github", "notion", "slack"]), query: z.string(), }) ), results: new ReducedValue( z.array(AgentOutput).default(() => []), { reducer: (current, update) => current.concat(update) } ), finalAnswer: z.string(), }); const searchCode = tool( async ({ query, repo }) => { return `Found code matching '${query}' in ${repo || "main"}: authentication middleware in src/auth.py`; }, { name: "search_code", description: "Search code in GitHub repositories.", schema: z.object({ query: z.string(), repo: z.string().optional().default("main"), }), } ); const searchIssues = tool( async ({ query }) => { return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`; }, { name: "search_issues", description: "Search GitHub issues and pull requests.", schema: z.object({ query: z.string(), }), } ); const searchPrs = tool( async ({ query }) => { return `PR #156 added JWT authentication, PR #178 updated OAuth scopes`; }, { name: "search_prs", description: "Search pull requests for implementation details.", schema: z.object({ query: z.string(), }), } ); const searchNotion = tool( async ({ query }) => { return `Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens`; }, { name: "search_notion", description: "Search Notion workspace for documentation.", schema: z.object({ query: z.string(), }), } ); const getPage = tool( async ({ pageId }) => { return `Page content: Step-by-step authentication setup instructions`; }, { name: "get_page", description: "Get a specific Notion page by ID.", schema: z.object({ pageId: z.string(), }), } ); const searchSlack = tool( async ({ query }) => { return `Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'`; }, { name: "search_slack", description: "Search Slack messages and threads.", schema: z.object({ query: z.string(), }), } ); const getThread = tool( async ({ threadId }) => { return `Thread discusses best practices for API key rotation`; }, { name: "get_thread", description: "Get a specific Slack thread.", schema: z.object({ threadId: z.string(), }), } ); import { createAgent } from "langchain"; import { ChatOpenAI } from "@langchain/openai"; const llm = new ChatOpenAI({ model: "gpt-4.1" }); const githubAgent = createAgent({ model: llm, tools: [searchCode, searchIssues, searchPrs], systemPrompt: ` You are a GitHub expert. Answer questions about code, API references, and implementation details by searching repositories, issues, and pull requests. `.trim(), }); const notionAgent = createAgent({ model: llm, tools: [searchNotion, getPage], systemPrompt: ` You are a Notion expert. Answer questions about internal processes, policies, and team documentation by searching the organization's Notion workspace. `.trim(), }); const slackAgent = createAgent({ model: llm, tools: [searchSlack, getThread], systemPrompt: ` You are a Slack expert. Answer questions by searching relevant threads and discussions where team members have shared knowledge and solutions. `.trim(), }); const routerLlm = new ChatOpenAI({ model: "gpt-4.1-mini" }); // Define structured output schema for the classifier const ClassificationResultSchema = z.object({ // [!code highlight] classifications: z .array( z.object({ source: z.enum(["github", "notion", "slack"]), query: z.string(), }) ) .describe("List of agents to invoke with their targeted sub-questions"), }); async function classifyQuery(state: typeof RouterState.State) { const structuredLlm = routerLlm.withStructuredOutput( ClassificationResultSchema ); // [!code highlight] const result = await structuredLlm.invoke([ { role: "system", content: `Analyze this query and determine which knowledge bases to consult. For each relevant source, generate a targeted sub-question optimized for that source. Available sources: - github: Code, API references, implementation details, issues, pull requests - notion: Internal documentation, processes, policies, team wikis - slack: Team discussions, informal knowledge sharing, recent conversations Return ONLY the sources that are relevant to the query. Each source should have a targeted sub-question optimized for that specific knowledge domain. Example for "How do I authenticate API requests?": - github: "What authentication code exists? Search for auth middleware, JWT handling" - notion: "What authentication documentation exists? Look for API auth guides" (slack omitted because it's not relevant for this technical question)`, }, { role: "user", content: state.query }, ]); return { classifications: result.classifications }; } function routeToAgents(state: typeof RouterState.State): Send[] { return state.classifications.map( (c) => new Send(c.source, { query: c.query }) // [!code highlight] ); } async function queryGithub(state: typeof RouterState.State) { const result = await githubAgent.invoke({ messages: [{ role: "user", content: state.query }], // [!code highlight] }); return { results: [{ source: "github", result: result.messages.at(-1)?.content }], }; } async function queryNotion(state: typeof RouterState.State) { const result = await notionAgent.invoke({ messages: [{ role: "user", content: state.query }], // [!code highlight] }); return { results: [{ source: "notion", result: result.messages.at(-1)?.content }], }; } async function querySlack(state: typeof RouterState.State) { const result = await slackAgent.invoke({ messages: [{ role: "user", content: state.query }], // [!code highlight] }); return { results: [{ source: "slack", result: result.messages.at(-1)?.content }], }; } async function synthesizeResults(state: typeof RouterState.State) { if (state.results.length === 0) { return { finalAnswer: "No results found from any knowledge source." }; } // Format results for synthesis const formatted = state.results.map( (r) => `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\n${r.result}` ); const synthesisResponse = await routerLlm.invoke([ { role: "system", content: `Synthesize these search results to answer the original question: "${state.query}" - Combine information from multiple sources without redundancy - Highlight the most relevant and actionable information - Note any discrepancies between sources - Keep the response concise and well-organized`, }, { role: "user", content: formatted.join("\n\n") }, ]); return { finalAnswer: synthesisResponse.content }; } const workflow = new StateGraph(RouterState) .addNode("classify", classifyQuery) .addNode("github", queryGithub) .addNode("notion", queryNotion) .addNode("slack", querySlack) .addNode("synthesize", synthesizeResults) .addEdge(START, "classify") .addConditionalEdges("classify", routeToAgents, ["github", "notion", "slack"]) .addEdge("github", "synthesize") .addEdge("notion", "synthesize") .addEdge("slack", "synthesize") .addEdge("synthesize", END) .compile(); const result = await workflow.invoke({ query: "How do I authenticate API requests?", }); console.log("Original query:", result.query); console.log("\nClassifications:"); for (const c of result.classifications) { console.log(` ${c.source}: ${c.query}`); } console.log(`\n${"=".repeat(60)}\n`); console.log("Final Answer:"); console.log(result.finalAnswer); ``` 

## 9. Advanced: Stateful routers

The router we've built so far is **stateless**â€”each request is handled independently with no memory between calls. For multi-turn conversations, you need a **stateful** approach.

### Tool wrapper approach

The simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:

```typescript  theme={null}
import { MemorySaver } from "@langchain/langgraph";

const searchKnowledgeBase = tool(
  async ({ query }) => {
    const result = await workflow.invoke({ query });
    return result.finalAnswer;
  },
  {
    name: "search_knowledge_base",
    description: `Search across multiple knowledge sources (GitHub, Notion, Slack).
Use this to find information about code, documentation, or team discussions.`,
    schema: z.object({
      query: z.string().describe("The search query"),
    }),
  }
);

const conversationalAgent = createAgent({
  model: llm,
  tools: [searchKnowledgeBase],
  systemPrompt: `
You are a helpful assistant that answers questions about our organization.
Use the search_knowledge_base tool to find information across our code,
documentation, and team discussions.
  `.trim(),
  checkpointer: new MemorySaver(),
});
```

This approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.

```typescript  theme={null}
const config = { configurable: { thread_id: "user-123" } };
let conversationalAgentResult = await conversationalAgent.invoke(
  {
    messages: [
      { role: "user", content: "How do I authenticate API requests?" },
    ],
  },
  config
);
console.log(conversationalAgentResult.messages.at(-1)?.content);

conversationalAgentResult = await conversationalAgent.invoke(
  {
    messages: [
      {
        role: "user",
        content: "What about rate limiting for those endpoints?",
      },
    ],
  },
  config
);
console.log(conversationalAgentResult.messages.at(-1)?.content);
```

 The tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory. 

### Full persistence approach

If you need the router itself to maintain stateâ€”for example, to use previous search results in routing decisionsâ€”use [persistence](/oss/javascript/langchain/short-term-memory) to store message history at the router level.

 **Stateful routers add complexity.** When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the [handoffs pattern](/oss/javascript/langchain/multi-agent/handoffs) or [subagents pattern](/oss/javascript/langchain/multi-agent/subagents) insteadâ€”both provide clearer semantics for multi-turn conversations with different agents. 

## 10. Key takeaways

The router pattern excels when you have:

* **Distinct verticals**: Separate knowledge domains that each require specialized tools and prompts
* **Parallel query needs**: Questions that benefit from querying multiple sources simultaneously
* **Synthesis requirements**: Results from multiple sources need to be combined into a coherent response

The pattern has three phases: **decompose** (analyze the query and generate targeted sub-questions), **route** (execute queries in parallel), and **synthesize** (combine results).

 **When to use the router pattern** Use the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic. For simpler cases with dynamic tool selection, consider the [subagents pattern](/oss/javascript/langchain/multi-agent/subagents). For workflows where agents need to converse with users sequentially, consider [handoffs](/oss/javascript/langchain/multi-agent/handoffs). 

## Next steps

* Learn about [handoffs](/oss/javascript/langchain/multi-agent/handoffs) for agent-to-agent conversations
* Explore the [subagents pattern](/oss/javascript/langchain/multi-agent/subagents-personal-assistant) for centralized orchestration
* Read the [multi-agent overview](/oss/javascript/langchain/multi-agent) to compare different patterns
* Use [LangSmith](https://smith.langchain.com) to debug and monitor your router

***

 [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/router-knowledge-base.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose). 

 [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
