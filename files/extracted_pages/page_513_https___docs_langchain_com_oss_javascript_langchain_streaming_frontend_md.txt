Published Time: Thu, 12 Feb 2026 11:17:55 GMT

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.langchain.com/llms.txt
> Use this file to discover all available pages before exploring further.

# Frontend

> Build generative UIs with real-time streaming from LangChain agents, LangGraph graphs, and custom APIs

The `useStream` React hook provides seamless integration with LangGraph streaming capabilities. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great generative UI experiences.

Key features:

*  **Messages streaming** â€” Handle a stream of message chunks to form a complete message *  **Automatic state management** â€” for messages, interrupts, loading states, and errors *  **Conversation branching** â€” Create alternate conversation paths from any point in the chat history *  **UI-agnostic design** â€” Bring your own components and styling ## Installation Install the LangGraph SDK to use the `useStream` hook in your React application: ```bash theme={null} npm install @langchain/langgraph-sdk ``` ## Basic usage The `useStream` hook connects to any LangGraph graph, whether that's running on from your own endpoint, or deployed using [LangSmith deployments](/langsmith/deployments). ```tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; function Chat() { const stream = useStream({ assistantId: "agent", // Local development apiUrl: "http://localhost:2024", // Production deployment (LangSmith hosted) // apiUrl: "https://your-deployment.us.langgraph.app" }); const handleSubmit = (message: string) => { stream.submit({ messages: [ { content: message, type: "human" } ], }); }; return ( 

 {stream.messages.map((message, idx) => ( 

 {message.type}: {message.content} 

 ))} {stream.isLoading && 

Loading...

} {stream.error && 

Error: {stream.error.message}

} 

 ); } ```  Learn how to [deploy your agents to LangSmith](/oss/javascript/langchain/deploy) for production-ready hosting with built-in observability, authentication, and scaling.   The ID of the agent to connect to. When using LangSmith deployments, this must match the agent ID shown in your deployment dashboard. For custom API deployments or local development, this can be any string that your server uses to identify the agent.   The URL of the LangGraph server. Defaults to `http://localhost:2024` for local development.   API key for authentication. Required when connecting to deployed agents on LangSmith.   Connect to an existing thread instead of creating a new one. Useful for resuming conversations.   Callback invoked when a new thread is created. Use this to persist the thread ID for later use.   Automatically resume an ongoing run when the component mounts. Set to `true` to use session storage, or provide a custom storage function.   Callback invoked when a new run is created. Useful for persisting run metadata for resumption.   Callback invoked when an error occurs during streaming.   Callback invoked when the stream completes successfully with the final state.   Handle custom events emitted from your agent using the `writer`. See [Custom streaming events](#custom-streaming-events).   Handle state update events after each graph step.   Handle metadata events with run and thread information.   The key in the graph state that contains the messages array.   Batch state updates for better rendering performance. Disable for immediate updates.   Initial state values to display while the first stream is loading. Useful for showing cached thread data immediately.   All messages in the current thread, including both human and AI messages.   The current graph state values. Type is inferred from the agent or graph type parameter.   Whether a stream is currently in progress. Use this to show loading indicators.   Any error that occurred during streaming. `null` when no error.   Current interrupt requiring user input, such as human-in-the-loop approval requests.   All tool calls across all messages, with their results and state (`pending`, `completed`, or `error`).   Submit new input to the agent. Pass `null` as input when resuming from an interrupt with a command. Options include `checkpoint` for branching, `optimisticValues` for optimistic updates, and `threadId` for optimistic thread creation.   Stop the current stream immediately.   Resume an existing stream by run ID. Use with `onCreated` for manual stream resumption.   Switch to a different branch in the conversation history.   Get all tool calls for a specific AI message.   Get metadata for a message, including streaming info like `langgraph_node` for identifying the source node, and `firstSeenState` for branching.   Tree representation of the thread for advanced branching controls in non-message based graphs.  ## Thread management Keep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created: ```tsx theme={null} import { useState } from "react"; import { useStream } from "@langchain/langgraph-sdk/react"; function Chat() { const [threadId, setThreadId] = useState(null); const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", threadId: threadId, onThreadId: setThreadId, }); // threadId is updated when a new thread is created // Store it in URL params or localStorage for persistence } ``` We recommend storing the `threadId` to let users resume conversations after page refreshes. ### Resume after page refresh The `useStream` hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost. ```tsx theme={null} const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", reconnectOnMount: true, }); ``` By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage function: ```tsx theme={null} const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", reconnectOnMount: () => window.localStorage, }); ``` For manual control over the resumption process, use the run callbacks to persist metadata and `joinStream` to resume: ```tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import { useEffect, useRef } from "react"; function Chat({ threadId }: { threadId: string | null }) { const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", threadId, onCreated: (run) => { // Persist run ID when stream starts window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id); }, onFinish: (_, run) => { // Clean up when stream completes window.sessionStorage.removeItem(`resume:${run?.thread_id}`); }, }); // Resume stream on mount if there's a stored run ID const joinedThreadId = useRef(null); useEffect(() => { if (!threadId) return; const runId = window.sessionStorage.getItem(`resume:${threadId}`); if (runId && joinedThreadId.current !== threadId) { stream.joinStream(runId); joinedThreadId.current = threadId; } }, [threadId]); const handleSubmit = (text: string) => { // Use streamResumable to ensure events aren't lost stream.submit( { messages: [{ type: "human", content: text }] }, { streamResumable: true } ); }; } ```  See a complete implementation of stream resumption with `reconnectOnMount` and thread persistence in the `session-persistence` example.  ## Optimistic updates You can optimistically update the client state before performing a network request, providing immediate feedback to the user: ```tsx theme={null} const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", }); const handleSubmit = (text: string) => { const newMessage = { type: "human" as const, content: text }; stream.submit( { messages: [newMessage] }, { optimisticValues(prev) { const prevMessages = prev.messages ?? []; return { ...prev, messages: [...prevMessages, newMessage] }; }, } ); }; ``` ### Optimistic thread creation Use the `threadId` option in `submit` to enable optimistic UI patterns where you need to know the thread ID before the thread is created: ```tsx theme={null} import { useState } from "react"; import { useStream } from "@langchain/langgraph-sdk/react"; function Chat() { const [threadId, setThreadId] = useState(null); const [optimisticThreadId] = useState(() => crypto.randomUUID()); const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", threadId, onThreadId: setThreadId, }); const handleSubmit = (text: string) => { // Navigate immediately without waiting for thread creation window.history.pushState({}, "", `/threads/${optimisticThreadId}`); // Create thread with the predetermined ID stream.submit( { messages: [{ type: "human", content: text }] }, { threadId: optimisticThreadId } ); }; } ``` ### Cached thread display Use the `initialValues` option to display cached thread data immediately while the history is being loaded from the server: ```tsx theme={null} function Chat({ threadId, cachedData }) { const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", threadId, initialValues: cachedData?.values, }); // Shows cached messages instantly, then updates when server responds } ``` ## Branching Create alternate conversation paths by editing previous messages or regenerating AI responses. Use `getMessagesMetadata()` to access checkpoint information for branching:  ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import { BranchSwitcher } from "./BranchSwitcher"; function Chat() { const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", }); return ( 

 {stream.messages.map((message) => { const meta = stream.getMessagesMetadata(message); const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint; return ( 

 

{message.content as string}

 {/* Edit human messages */} {message.type === "human" && (  { const newContent = prompt("Edit message:", message.content as string); if (newContent) { stream.submit( { messages: [{ type: "human", content: newContent }] }, { checkpoint: parentCheckpoint } ); } }} > Edit  )} {/* Regenerate AI messages */} {message.type === "ai" && (  stream.submit(undefined, { checkpoint: parentCheckpoint })} > Regenerate  )} {/* Switch between branches */}  stream.setBranch(branch)} /> 

 ); })} 

 ); } ``` ```tsx BranchSwitcher.tsx theme={null} /** * Component for navigating between conversation branches. * Shows the current branch position and allows switching between alternatives. */ export function BranchSwitcher({ branch, branchOptions, onSelect, }: { branch: string | undefined; branchOptions: string[] | undefined; onSelect: (branch: string) => void; }) { if (!branchOptions || !branch) return null; const index = branchOptions.indexOf(branch); return ( 

  onSelect(branchOptions[index - 1])} > â†  {index + 1} / {branchOptions.length} = branchOptions.length - 1} onClick={() => onSelect(branchOptions[index + 1])} > â†’  

 ); } ```  For advanced use cases, use the `experimental_branchTree` property to get the tree representation of the thread for non-message based graphs.  See a complete implementation of conversation branching with edit, regenerate, and branch switching in the `branching-chat` example.  ## Type-safe streaming The `useStream` hook supports full type inference when used with agents created via [`createAgent`](https://reference.langchain.com/javascript/functions/langchain.index.createAgent.html) or graphs created with [`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html). Pass `typeof agent` or `typeof graph` as the type parameter to automatically infer tool call types. ### With `createAgent` When using [`createAgent`](https://reference.langchain.com/javascript/functions/langchain.index.createAgent.html), tool call types are automatically inferred from the tools you register to your agent:  ```typescript agent.ts theme={null} import { createAgent, tool } from "langchain"; import { z } from "zod"; const getWeather = tool( async ({ location }) => `Weather in ${location}: Sunny, 72Â°F`, { name: "get_weather", description: "Get weather for a location", schema: z.object({ location: z.string().describe("The city to get weather for"), }), } ); export const agent = createAgent({ model: "openai:gpt-4.1-mini", tools: [getWeather], }); ``` ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import type { agent } from "./agent"; function Chat() { // Tool calls are automatically typed from the agent's tools const stream = useStream({ assistantId: "agent", apiUrl: "http://localhost:2024", }); // stream.toolCalls[0].call.name is typed as "get_weather" // stream.toolCalls[0].call.args is typed as { location: string } } ```  ### With `StateGraph` For custom [`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) applications, the state types are inferred from the graph's annotation:  ```typescript graph.ts theme={null} import { StateGraph, MessagesAnnotation, START, END } from "@langchain/langgraph"; import { ChatOpenAI } from "@langchain/openai"; const model = new ChatOpenAI({ model: "gpt-4.1-mini" }); const workflow = new StateGraph(MessagesAnnotation) .addNode("agent", async (state) => { const response = await model.invoke(state.messages); return { messages: [response] }; }) .addEdge(START, "agent") .addEdge("agent", END); export const graph = workflow.compile(); ``` ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import type { graph } from "./graph"; function Chat() { // State types are automatically inferred from the graph const stream = useStream({ assistantId: "my-graph", apiUrl: "http://localhost:2024", }); // stream.values is typed based on the graph's state annotation } ```  ### With Annotation types If you're using LangGraph.js, you can reuse your graph's annotation types. Make sure to only import types to avoid importing the entire LangGraph.js runtime: ```tsx theme={null} import { Annotation, MessagesAnnotation, type StateType, type UpdateType, } from "@langchain/langgraph/web"; const AgentState = Annotation.Root({ ...MessagesAnnotation.spec, context: Annotation(), }); const stream = useStream< StateType, { UpdateType: UpdateType } >({ apiUrl: "http://localhost:2024", assistantId: "agent", }); ``` ### Advanced type configuration You can specify additional type parameters for interrupts, custom events, and configurable options: ```tsx theme={null} import type { Message } from "@langchain/langgraph-sdk"; type State = { messages: Message[]; context?: string }; const stream = useStream< State, { UpdateType: { messages: Message[] | Message; context?: string }; InterruptType: string; CustomEventType: { type: "progress" | "debug"; payload: unknown }; ConfigurableType: { model: string }; } >({ apiUrl: "http://localhost:2024", assistantId: "agent", }); // stream.interrupt is typed as string | undefined // onCustomEvent receives typed events ``` ## Rendering tool calls Use `getToolCalls` to extract and render tool calls from AI messages. Tool calls include the call details, result (if completed), and state.  ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import type { agent } from "./agent"; import { ToolCallCard } from "./ToolCallCard"; import { MessageBubble } from "./MessageBubble"; function Chat() { const stream = useStream({ assistantId: "agent", apiUrl: "http://localhost:2024", }); return ( 

 {stream.messages.map((message, idx) => { if (message.type === "ai") { const toolCalls = stream.getToolCalls(message); if (toolCalls.length > 0) { return ( 

 {toolCalls.map((toolCall) => (  ))} 

 ); } } return ; })} 

 ); } ``` ```tsx ToolCallCard.tsx theme={null} import type { ToolCallWithResult, ToolCallFromTool, ToolCallState, InferAgentToolCalls, } from "@langchain/langgraph-sdk/react"; import type { ToolMessage } from "@langchain/langgraph-sdk"; import type { agent } from "./agent"; import type { getWeather } from "./tools"; import { parseToolResult } from "./utils"; import { WeatherCard } from "./WeatherCard"; /** * Define tool call types for this component. * Use InferAgentToolCalls for agents or ToolCallFromTool for individual tools. */ type AgentToolCalls = InferAgentToolCalls; /** * Component that renders a tool call with its result. * Uses typed ToolCallWithResult for discriminated union narrowing. */ export function ToolCallCard({ toolCall, }: { toolCall: ToolCallWithResult; }) { const { call, result, state } = toolCall; // Type narrowing works when call.name is a literal type if (call.name === "get_weather") { return ; } // Fallback for other tools return ; } ``` ```tsx GenericToolCallCard.tsx theme={null} import type { ToolCallState } from "@langchain/langgraph-sdk/react"; import type { ToolMessage } from "@langchain/langgraph-sdk"; import { parseToolResult } from "./utils"; /** * Generic fallback for unknown or unhandled tools. * Uses a simple type that works with any tool call. */ export function GenericToolCallCard({ call, result, state, }: { call: { name: string; args: Record }; result?: ToolMessage; state: ToolCallState; }) { const isLoading = state === "pending"; const parsedResult = parseToolResult(result); return ( 

 

 

 

 {call.name} 

 

 {isLoading ? "Processing..." : "Completed"} 

 

 

 
          {JSON.stringify(call.args, null, 2)}
         {result && ( 

 {parsedResult.content} 

 )} 

 ); } ``` ```tsx WeatherCard.tsx theme={null} import type { ToolCallFromTool, ToolCallState } from "@langchain/langgraph-sdk/react"; import type { ToolMessage } from "@langchain/langgraph-sdk"; import type { getWeather } from "./tools"; import { parseToolResult } from "./utils"; // Infer tool call type directly from the tool definition type GetWeatherToolCall = ToolCallFromTool; /** * Weather-specific tool card with rich UI. * Uses ToolCallFromTool to infer args type from the tool schema. */ export function WeatherCard({ call, result, state, }: { call: GetWeatherToolCall; result?: ToolMessage; state: ToolCallState; }) { const isLoading = state === "pending"; const parsedResult = parseToolResult(result); return ( 

 {/* Sky gradient background */} 

 {/* call.args is typed as { location: string } from the tool schema */} {call.args.location} {isLoading && Loading...} 

 {parsedResult.status === "error" ? ( 

 {parsedResult.content} 

 ) : ( 

 {parsedResult.content || "Fetching weather..."} 

 )} 

 ); } ``` ```typescript tools.ts theme={null} import { tool } from "@langchain/core/tools"; import { z } from "zod"; // Define the weather tool with a Zod schema export const getWeather = tool( async ({ location }) => { // Tool implementation return JSON.stringify({ status: "success", content: `Weather in ${location}: Sunny, 72Â°F` }); }, { name: "get_weather", description: "Get the current weather for a location", schema: z.object({ location: z.string().describe("The city and state, e.g. San Francisco, CA"), }), } ); ``` ```typescript utils.ts theme={null} import type { ToolMessage } from "@langchain/langgraph-sdk"; /** * Helper to parse tool result safely. * Tool results may be JSON strings or plain text. */ export function parseToolResult(result?: ToolMessage): { status: string; content: string; } { if (!result) return { status: "pending", content: "" }; try { return JSON.parse(result.content as string); } catch { return { status: "success", content: result.content as string }; } } ``` 

 See a complete implementation of tool call rendering with weather, calculator, and note-taking tools in the `tool-calling-agent` example.  ## Custom streaming events Stream custom data from your agent using the `writer` in your tools or nodes. Handle these events in the UI with the `onCustomEvent` callback.  ```typescript agent.ts theme={null} import { tool, type ToolRuntime } from "langchain"; import { z } from "zod"; // Define your custom event types interface ProgressData { type: "progress"; id: string; message: string; progress: number; } const analyzeDataTool = tool( async ({ dataSource }, config: ToolRuntime) => { const steps = ["Connecting...", "Fetching...", "Processing...", "Done!"]; for (let i = 0; i < steps.length; i++) { // Emit progress events during execution config.writer?.({ type: "progress", id: `analysis-${Date.now()}`, message: steps[i], progress: ((i + 1) / steps.length) * 100, } satisfies ProgressData); await new Promise((resolve) => setTimeout(resolve, 500)); } return JSON.stringify({ result: "Analysis complete" }); }, { name: "analyze_data", description: "Analyze data with progress updates", schema: z.object({ dataSource: z.string().describe("Data source to analyze"), }), } ); ``` ```tsx Chat.tsx theme={null} import { useState, useCallback } from "react"; import { useStream } from "@langchain/langgraph-sdk/react"; import type { agent } from "./agent"; interface ProgressData { type: "progress"; id: string; message: string; progress: number; } function isProgressData(data: unknown): data is ProgressData { return ( typeof data === "object" && data !== null && "type" in data && (data as ProgressData).type === "progress" ); } function CustomStreamingUI() { const [progressData, setProgressData] = useState>( new Map() ); const handleCustomEvent = useCallback((data: unknown) => { if (isProgressData(data)) { setProgressData((prev) => { const updated = new Map(prev); updated.set(data.id, data); return updated; }); } }, []); const stream = useStream({ assistantId: "custom-streaming", apiUrl: "http://localhost:2024", onCustomEvent: handleCustomEvent, }); return ( 

 {/* Render progress cards */} {Array.from(progressData.values()).map((data) => ( 

{data.message}{data.progress}%

 ))} 

 ); } ``` 

 See a complete implementation of custom events with progress bars, status badges, and file operation cards in the `custom-streaming` example.  ## Event handling The `useStream` hook provides callback options that give you access to different types of streaming events. You don't need to explicitly configure stream modesâ€”just pass callbacks for the event types you want to handle: ```tsx theme={null} const stream = useStream({ apiUrl: "http://localhost:2024", assistantId: "agent", // Handle state updates after each graph step onUpdateEvent: (update, options) => { console.log("Graph update:", update); }, // Handle custom events streamed from your graph onCustomEvent: (event, options) => { console.log("Custom event:", event); }, // Handle metadata events with run/thread info onMetadataEvent: (metadata) => { console.log("Run ID:", metadata.run_id); console.log("Thread ID:", metadata.thread_id); }, onError: (error) => { console.error("Stream error:", error); }, onFinish: (state, options) => { console.log("Stream finished with final state:", state); }, }); ``` ### Available callbacks | Callback | Description | Stream mode | | ----------------- | ------------------------------------------------------------ | ----------- | | `onUpdateEvent` | Called when a state update is received after each graph step | `updates` | | `onCustomEvent` | Called when a custom event is received from your graph | `custom` | | `onMetadataEvent` | Called with run and thread metadata | `metadata` | | `onError` | Called when an error occurs | - | | `onFinish` | Called when the stream completes | - | ## Multi-agent streaming When working with multi-agent systems or graphs with multiple nodes, use message metadata to identify which node generated each message. This is particularly useful when multiple LLMs run in parallel and you want to display their outputs with distinct visual styling.  ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import type { agent } from "./agent"; import { MessageBubble } from "./MessageBubble"; // Node configuration for visual display const NODE_CONFIG: Record = { researcher_analytical: { label: "Analytical Research", color: "cyan" }, researcher_creative: { label: "Creative Research", color: "purple" }, researcher_practical: { label: "Practical Research", color: "emerald" }, }; function MultiAgentChat() { const stream = useStream({ assistantId: "parallel-research", apiUrl: "http://localhost:2024", }); return ( 

 {stream.messages.map((message, idx) => { if (message.type !== "ai") { return ; } // Get streaming metadata to identify the source node const metadata = stream.getMessagesMetadata?.(message); const nodeName = (metadata?.streamMetadata?.langgraph_node as string) || (message as { name?: string }).name; const config = nodeName ? NODE_CONFIG[nodeName] : null; if (!config) { return ; } return ( 

 {config.label} 

 {typeof message.content === "string" ? message.content : ""} 

 ); })} 

 ); } ``` ```typescript agent.ts theme={null} import { ChatOpenAI } from "@langchain/openai"; import { StateGraph, START, END, Send, StateSchema, MessagesValue, GraphNode, ConditionalEdgeRouter, } from "@langchain/langgraph"; import { AIMessage } from "@langchain/core/messages"; import { z } from "zod"; // Use different model instances for variety const analyticalModel = new ChatOpenAI({ model: "gpt-4.1-mini", temperature: 0.3 }); const creativeModel = new ChatOpenAI({ model: "gpt-4.1-mini", temperature: 0.9 }); const practicalModel = new ChatOpenAI({ model: "gpt-4.1-mini", temperature: 0.5 }); // Define the state schema const StateAnnotation = new StateSchema({ messages: MessagesValue, topic: z.string().default(""), analyticalResearch: z.string().default(""), creativeResearch: z.string().default(""), practicalResearch: z.string().default(""), }); type State = typeof StateAnnotation.State; // Fan-out to parallel researchers const fanOutToResearchers: ConditionalEdgeRouter = (state) => { return [ new Send("researcher_analytical", state), new Send("researcher_creative", state), new Send("researcher_practical", state), ]; }; const dispatcherNode: GraphNode = async (state) => { const lastMessage = state.messages.at(-1); const topic = typeof lastMessage?.content === "string" ? lastMessage.content : ""; return { topic }; }; const analyticalResearcherNode: GraphNode = async (state) => { const response = await analyticalModel.invoke([ { role: "system", content: "You are an analytical research expert. Focus on data and evidence." }, { role: "user", content: `Research: ${state.topic}` }, ]); return { analyticalResearch: response.content as string, messages: [new AIMessage({ content: response.content as string, name: "researcher_analytical" })], }; }; // Similar nodes for creative and practical researchers... // Build the graph with parallel execution const workflow = new StateGraph(StateAnnotation) .addNode("dispatcher", dispatcherNode) .addNode("researcher_analytical", analyticalResearcherNode) .addNode("researcher_creative", creativeResearcherNode) .addNode("researcher_practical", practicalResearcherNode) .addEdge(START, "dispatcher") .addConditionalEdges("dispatcher", fanOutToResearchers) .addEdge("researcher_analytical", END) .addEdge("researcher_creative", END) .addEdge("researcher_practical", END); export const agent = workflow.compile(); ```  See a complete implementation of multi-agent streaming with three parallel researchers and distinct visual styling in the `parallel-research` example.  ## Human-in-the-loop Handle interrupts when the agent requires human approval for tool execution. Learn more in the [How to handle interrupts](/oss/javascript/langgraph/interrupts#pause-using-interrupt) guide.  ```tsx Chat.tsx theme={null} import { useState } from "react"; import { useStream } from "@langchain/langgraph-sdk/react"; import type { HITLRequest, HITLResponse } from "langchain"; import type { agent } from "./agent"; import { MessageBubble } from "./MessageBubble"; function HumanInTheLoopChat() { const stream = useStream({ assistantId: "human-in-the-loop", apiUrl: "http://localhost:2024", }); const [isProcessing, setIsProcessing] = useState(false); // Type assertion for interrupt value const hitlRequest = stream.interrupt?.value as HITLRequest | undefined; const handleApprove = async (index: number) => { if (!hitlRequest) return; setIsProcessing(true); try { const decisions: HITLResponse["decisions"] = hitlRequest.actionRequests.map((_, i) => i === index ? { type: "approve" } : { type: "approve" } ); await stream.submit(null, { command: { resume: { decisions } as HITLResponse, }, }); } finally { setIsProcessing(false); } }; const handleReject = async (index: number, reason: string) => { if (!hitlRequest) return; setIsProcessing(true); try { const decisions: HITLResponse["decisions"] = hitlRequest.actionRequests.map((_, i) => i === index ? { type: "reject", message: reason } : { type: "reject", message: "Rejected along with other actions" } ); await stream.submit(null, { command: { resume: { decisions } as HITLResponse, }, }); } finally { setIsProcessing(false); } }; return ( 

 {/* Render messages */} {stream.messages.map((message, idx) => (  ))} {/* Render approval UI when interrupted */} {hitlRequest && hitlRequest.actionRequests.length > 0 && ( 

### Action requires approval

 {hitlRequest.actionRequests.map((action, idx) => ( 

 {action.name} 

                  {JSON.stringify(action.args, null, 2)}
                

 handleApprove(idx)} disabled={isProcessing} className="px-3 py-1.5 bg-green-600 hover:bg-green-700 text-white text-sm rounded disabled:opacity-50" > Approve  handleReject(idx, "User rejected")} disabled={isProcessing} className="px-3 py-1.5 bg-red-600 hover:bg-red-700 text-white text-sm rounded disabled:opacity-50" > Reject 

 ))} 

 )} 

 ); } ``` ```typescript agent.ts theme={null} import { createAgent, tool, humanInTheLoopMiddleware } from "langchain"; import { ChatOpenAI } from "@langchain/openai"; import { MemorySaver } from "@langchain/langgraph"; import { z } from "zod"; const model = new ChatOpenAI({ model: "gpt-4.1-mini" }); // Tool that requires human approval const sendEmail = tool( async ({ to, subject, body }) => { return { status: "success", content: `Email sent to ${to} with subject "${subject}"`, }; }, { name: "send_email", description: "Send an email. Requires human approval.", schema: z.object({ to: z.string().describe("Recipient email address"), subject: z.string().describe("Email subject"), body: z.string().describe("Email body"), }), } ); // Tool that requires approval with limited options const deleteFile = tool( async ({ path }) => { return { status: "success", content: `File "${path}" deleted` }; }, { name: "delete_file", description: "Delete a file. Requires human approval.", schema: z.object({ path: z.string().describe("File path to delete"), }), } ); // Safe tool - no approval needed const readFile = tool( async ({ path }) => { return { status: "success", content: `Contents of ${path}...` }; }, { name: "read_file", description: "Read file contents. No approval needed.", schema: z.object({ path: z.string().describe("File path to read"), }), } ); // Create agent with HITL middleware export const agent = createAgent({ model, tools: [sendEmail, deleteFile, readFile], middleware: [ humanInTheLoopMiddleware({ interruptOn: { // Email requires all decision types send_email: { allowedDecisions: ["approve", "edit", "reject"], description: "ðŸ“§ Review email before sending", }, // Deletion only allows approve/reject delete_file: { allowedDecisions: ["approve", "reject"], description: "ðŸ—‘ï¸ Confirm file deletion", }, // Reading is safe - auto-approved read_file: false, }, }), ], // Required for HITL - persists state across interrupts checkpointer: new MemorySaver(), }); ```  See a complete implementation of approval workflows with approve, reject, and edit actions in the `human-in-the-loop` example.  ## Reasoning models  Extended reasoning/thinking support is currently experimental. The streaming interface for reasoning tokens varies by provider (OpenAI vs. Anthropic) and may change as abstractions are developed.  When using models with extended reasoning capabilities (like OpenAI's reasoning models or Anthropic's extended thinking), the thinking process is embedded in the message content. You'll need to extract and display it separately.  ```tsx Chat.tsx theme={null} import { useStream } from "@langchain/langgraph-sdk/react"; import type { Message } from "@langchain/langgraph-sdk"; import type { agent } from "./agent"; import { getReasoningFromMessage, getTextContent } from "./utils"; function ReasoningChat() { const stream = useStream({ assistantId: "reasoning-agent", apiUrl: "http://localhost:2024", }); return ( 

 {stream.messages.map((message, idx) => { if (message.type === "ai") { const reasoning = getReasoningFromMessage(message); const textContent = getTextContent(message); return ( 

 {/* Render reasoning bubble if present */} {reasoning && ( 

 Reasoning 

 {reasoning} 

 )} {/* Render text content */} {textContent && ( 

 {textContent} 

 )} 

 ); } return ; })} {stream.isLoading && ( 

Thinking...

 )} 

 ); } ``` ```typescript utils.ts theme={null} import type { Message, AIMessage } from "@langchain/langgraph-sdk"; /** * Extracts reasoning/thinking content from an AI message. * Supports both OpenAI reasoning (additional_kwargs.reasoning.summary) * and Anthropic extended thinking (content blocks with type "thinking"). */ export function getReasoningFromMessage(message: Message): string | undefined { type MessageWithExtras = AIMessage & { additional_kwargs?: { reasoning?: { summary?: Array<{ type: string; text: string }>; }; }; contentBlocks?: Array<{ type: string; thinking?: string }>; }; const msg = message as MessageWithExtras; // Check for OpenAI reasoning in additional_kwargs if (msg.additional_kwargs?.reasoning?.summary) { const content = msg.additional_kwargs.reasoning.summary .filter((item) => item.type === "summary_text") .map((item) => item.text) .join(""); if (content.trim()) return content; } // Check for Anthropic thinking in contentBlocks if (msg.contentBlocks?.length) { const thinking = msg.contentBlocks .filter((b) => b.type === "thinking" && b.thinking) .map((b) => b.thinking) .join("\n"); if (thinking) return thinking; } // Check for thinking in message.content array if (Array.isArray(msg.content)) { const thinking = msg.content .filter((b): b is { type: "thinking"; thinking: string } => typeof b === "object" && b?.type === "thinking" && "thinking" in b ) .map((b) => b.thinking) .join("\n"); if (thinking) return thinking; } return undefined; } /** * Extracts text content from a message. */ export function getTextContent(message: Message): string { if (typeof message.content === "string") return message.content; if (Array.isArray(message.content)) { return message.content .filter((c): c is { type: "text"; text: string } => c.type === "text") .map((c) => c.text) .join(""); } return ""; } ```  See a complete implementation of reasoning token display with OpenAI and Anthropic models in the `reasoning-agent` example.  ## Custom state types For custom LangGraph applications, embed your tool call types in your state's messages property. ```tsx theme={null} import { Message } from "@langchain/langgraph-sdk"; import { useStream } from "@langchain/langgraph-sdk/react"; // Define your tool call types as a discriminated union type MyToolCalls = | { name: "search"; args: { query: string }; id?: string } | { name: "calculate"; args: { expression: string }; id?: string }; // Embed tool call types in your state's messages interface MyGraphState { messages: Message[]; context?: string; } function CustomGraphChat() { const stream = useStream({ assistantId: "my-graph", apiUrl: "http://localhost:2024", }); // stream.values is typed as MyGraphState // stream.toolCalls[0].call.name is typed as "search" | "calculate" } ``` You can also specify additional type configuration for interrupts and configurable options: ```tsx theme={null} interface MyGraphState { messages: Message[]; } function CustomGraphChat() { const stream = useStream< MyGraphState, { InterruptType: { question: string }; ConfigurableType: { userId: string }; } >({ assistantId: "my-graph", apiUrl: "http://localhost:2024", }); // stream.interrupt is typed as { question: string } | undefined } ``` ## Custom transport For custom API endpoints or non-standard deployments, use the `transport` option with `FetchStreamTransport` to connect to any streaming API. ```tsx theme={null} import { useMemo } from "react"; import { useStream, FetchStreamTransport } from "@langchain/langgraph-sdk/react"; function CustomAPIChat({ apiKey }: { apiKey: string }) { // Create transport with custom request handling const transport = useMemo(() => { return new FetchStreamTransport({ apiUrl: "/api/my-agent", onRequest: async (url: string, init: RequestInit) => { // Inject API key or other custom data into requests const customBody = JSON.stringify({ ...(JSON.parse(init.body as string) || {}), apiKey, }); return { ...init, body: customBody, headers: { ...init.headers, "X-Custom-Header": "value", }, }; }, }); }, [apiKey]); const stream = useStream({ transport, }); // Use stream as normal return ( 

 {stream.messages.map((message, idx) => (  ))} 

 ); } ``` ## Related * [Streaming overview](/oss/javascript/langchain/streaming/overview) â€” Server-side streaming with LangChain agents * [useStream API Reference](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) â€” Full API documentation * [Agent Chat UI](/oss/javascript/langchain/ui) â€” Pre-built chat interface for LangGraph agents * [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) â€” Configuring interrupts for human review * [Multi-agent systems](/oss/javascript/langchain/multi-agent) â€” Building agents with multiple LLMs ***  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming/frontend.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
